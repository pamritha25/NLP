{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTOXISZSmzp0glk0cvH6ZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamritha25/NLP/blob/main/NLP0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP(NATURAL LANGUAGE PREPROCESSING)**"
      ],
      "metadata": {
        "id": "wCnJWQTO5KXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**to stop words**"
      ],
      "metadata": {
        "id": "Km7LD5YX5iw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "text = \"This is a sample sentence.\"\n",
        "words = word_tokenize(text)\n",
        "filtered = [w for w in words if w.lower() not in stopwords.words('english')]\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG-cMjKQ7oQp",
        "outputId": "a0ad59f8-ac35-4a7b-c333-dd7698e52f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenization"
      ],
      "metadata": {
        "id": "sATCZ8in6zBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize('This is a sentence'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv2B4OF07rHx",
        "outputId": "5311cc79-8ab5-4d0c-e4b5-c6a1c1accdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemmming"
      ],
      "metadata": {
        "id": "j0taYqsE79j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import porter\n",
        "stemmer=nltk.PorterStemmer()\n",
        "print(stemmer.stem(\"running\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcHmd2zL7_DS",
        "outputId": "53193821-9cd6-4802-d3dc-e3a3d66577f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "ADqqIdC_8QGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_kesxvm85kP",
        "outputId": "6de57336-f793-41be-a7eb-15de8bd7dcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'v' means verb\n",
        "\n",
        "'n' means noun (this is the default if you don’t specify pos)\n",
        "\n",
        "'a' means adjective"
      ],
      "metadata": {
        "id": "tZK01dx08-Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word1 = \"running\"\n",
        "word2 = \"better\"\n",
        "word3 = \"studies\"\n",
        "\n",
        "print(\"Stemming:\")\n",
        "print(stemmer.stem(word1))\n",
        "print(stemmer.stem(word2))\n",
        "print(stemmer.stem(word3))\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "print(lemmatizer.lemmatize(word1, pos='v'))\n",
        "print(lemmatizer.lemmatize(word2, pos='a'))\n",
        "print(lemmatizer.lemmatize(word3, pos='n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgF6xjD-8_J-",
        "outputId": "c0ffca16-5c60-42bf-c966-0822aa9e6676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "run\n",
            "better\n",
            "studi\n",
            "\n",
            "Lemmatization:\n",
            "run\n",
            "good\n",
            "study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS(parts of speech)**"
      ],
      "metadata": {
        "id": "0dVMcdIu9Lwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the  POS tagger data\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')  # Also needed for word_tokenize\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(\"This is a sample sentence\")\n",
        "\n",
        "# Get POS tags\n",
        "print(nltk.pos_tag(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLBh4vSi9RrJ",
        "outputId": "554791f4-e6ca-4226-ddd3-75e6d58b9f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER(Named entity recognition)"
      ],
      "metadata": {
        "id": "OFaYadxi9VxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Brack obama was born in Hawaai\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdVzDCjK9Yjw",
        "outputId": "bff9f028-e9ae-47e6-9b5f-d0c1588b05fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hawaai', 'ORG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "aQGYmHN39-Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Apple is looking at buying U.K startup for $1 billion\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpTkyfb_-CD6",
        "outputId": "5bc17a2c-f04e-41cd-a5e8-44b67746a5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('U.K', 'ORG'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text=\"The quick brown fox jumps over the lazy dog\"\n",
        "tokens=nltk.word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(chunk_grammar)\n",
        "tree = cp.parse(tags)\n",
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Mb1_Vl-n6Q",
        "outputId": "d1f0c0e8-91fb-4396-d750-5725bff646b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     S                                 \n",
            "     ________________________________|______________________            \n",
            "    |        |              NP               NP             NP         \n",
            "    |        |       _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entity recognition(NER)"
      ],
      "metadata": {
        "id": "EHzwSv9u-_Hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding or Vectorization"
      ],
      "metadata": {
        "id": "Ih_Vun82_MC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform([\"This is a sample\", \"This is another example\"])\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMlX6K9W_MxB",
        "outputId": "b4cf96eb-1036-48cb-9596-3f2365906e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 1]\n",
            " [1 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Index:   0    1      2      3        4\n",
        "- Words: ['another', 'example', 'is', 'sample', 'this']"
      ],
      "metadata": {
        "id": "FMEh-cvn_S4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSaoHvMo_TlU",
        "outputId": "007d0e71-ec05-4e3f-be85-d577d9f867b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['another' 'example' 'is' 'sample' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *TF-IDF"
      ],
      "metadata": {
        "id": "yZL4VU58_inI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TF = Term Frequency → How often a word appears in a document.\n",
        "\n",
        "- IDF = Inverse Document Frequency → Measures how important a word is across all documents.\n",
        "\n",
        " - Words that appear in many documents get down-weighted.\n",
        "\n",
        "- TF-IDF = TF × IDF → Higher value means the word is important for that document but rare across others."
      ],
      "metadata": {
        "id": "8GkOPxDu_wdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "X=tf.fit_transform([\"This is a sample\", \"This is another example\"])\n",
        "print(tf.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrqPoOcj_kHC",
        "outputId": "634e22cf-5b85-45fc-fbb3-2462ef45d453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['another' 'example' 'is' 'sample' 'this']\n",
            "[[0.         0.         0.50154891 0.70490949 0.50154891]\n",
            " [0.57615236 0.57615236 0.40993715 0.         0.40993715]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[[0.         0.         0.5        0.70710678 0.5       ]\n",
        " [0.5        0.5        0.5        0.         0.5       ]]\n",
        "Row 1: \"This is a sample\"\n",
        "\n",
        "'another': 0 (not in doc)\n",
        "\n",
        "'example': 0 (not in doc)\n",
        "\n",
        "'is': 0.5 (TF-IDF score)\n",
        "\n",
        "'sample': 0.7071 (TF-IDF score — more unique!)\n",
        "\n",
        "'this': 0.5\n",
        "\n",
        "Row 2: \"This is another example\"\n",
        "\n",
        "'another': 0.5\n",
        "\n",
        "'example': 0.5\n",
        "\n",
        "'is': 0.5\n",
        "\n",
        "'sample': 0\n",
        "\n",
        "'this': 0.5"
      ],
      "metadata": {
        "id": "AQ5iSFqBN5FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountVectorizer to extract n-grams**"
      ],
      "metadata": {
        "id": "wGvE_km4OAsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"I love NLP\"]\n",
        "cv = CountVectorizer(ngram_range=(1, 3))\n",
        "X = cv.fit_transform(text)\n",
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiLPgwFeN6el",
        "outputId": "226410fb-b414-4eed-dd2f-60d79d2a888b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love' 'love nlp' 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (1, 1) (default)\tOnly single words (unigrams)\n",
        "- (1, 2)\tUnigrams and bigrams\n",
        "- (2, 2)\tOnly bigrams\n",
        "- (1, 3)\tUnigrams, bigrams, and trigrams"
      ],
      "metadata": {
        "id": "KgKmrXY4ONOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sentiment analysis**"
      ],
      "metadata": {
        "id": "iXImJ5e9OYh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <b>Polarity: 0.5 </b> <br> <hr>\n",
        "Range: from -1.0 (most negative) to +1.0 (most positive) <br>\n",
        "Meaning: A value of 0.5 indicates a moderately positive sentiment. <br>\n",
        "Negative: < 0 <br>\n",
        "Neutral: 0 <br>\n",
        "Positive: > 0 <br>\n",
        " <b>Subjectivity: 0.6 </b> <br> <hr>\n",
        "Range: from 0.0 (objective) to 1.0 (subjective)<br>\n",
        "Meaning: A value of 0.6 suggests the text is fairly subjective, meaning it's based more on personal opinion or feelings than on factual information."
      ],
      "metadata": {
        "id": "71CXCRbBO-5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(\"I love this product\").sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLto7o1kOI4N",
        "outputId": "57e7f7dd-1626-47a4-e44c-9293cc055a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.5, subjectivity=0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term             | Simple meaning                     |\n",
        "| ---------------- | ---------------------------------- |\n",
        "| **Polarity**     | How negative or positive it is     |\n",
        "| **Subjectivity** | How much of it is opinion vs. fact |"
      ],
      "metadata": {
        "id": "skrvlb1EPZ7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term             | Range                      | Meaning                                      |\n",
        "| ---------------- | -------------------------- | -------------------------------------------- |\n",
        "| **Polarity**     | `-1.0` to `+1.0`           | How **negative** or **positive** the text is |\n",
        "|                  | `-1.0` → Very negative     | Example: *“I hate this movie.”*              |\n",
        "|                  | `0.0` → Neutral            | Example: *“It is a movie.”*                  |\n",
        "|                  | `+1.0` → Very positive     | Example: *“I love this movie!”*              |\n",
        "|                  |                            |                                              |\n",
        "| **Subjectivity** | `0.0` to `1.0`             | How much is **opinion** vs. **fact**         |\n",
        "|                  | `0.0` → Completely factual | Example: *“The sun rises in the east.”*      |\n",
        "|                  | `1.0` → Fully opinionated  | Example: *“I think this is the best ever!”*  |"
      ],
      "metadata": {
        "id": "J2_sIjz7PnKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Classification with Pipeline**"
      ],
      "metadata": {
        "id": "T6QyAQ1WPxeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#  Example text data (spam detection style)\n",
        "texts = [\n",
        "    \"Buy now\",\n",
        "    \"Limited offer\",\n",
        "    \"Call me later\",\n",
        "    \"Meeting at noon\",\n",
        "    \"Free coupons\",\n",
        "    \"Let’s have lunch\",\n",
        "    \"Win a prize now\",\n",
        "    \"Project discussion tomorrow\"\n",
        "]\n",
        "labels = [\n",
        "    \"spam\",  # Buy now\n",
        "    \"spam\",  # Limited offer\n",
        "    \"ham\",   # Call me later\n",
        "    \"ham\",   # Meeting at noon\n",
        "    \"spam\",  # Free coupons\n",
        "    \"ham\",   # Let’s have lunch\n",
        "    \"spam\",  # Win a prize now\n",
        "    \"ham\"    # Project discussion tomorrow\n",
        "]\n",
        "\n",
        "#  Split into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "#  Create the pipeline\n",
        "pipe = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "#  Train the model\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "preds = pipe.predict(X_test)\n",
        "\n",
        "#  Evaluate the model\n",
        "print(\"Test predictions:\", preds)\n",
        "print(\"Actual labels:   \", y_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "\n",
        "#  Try a new example\n",
        "new_text = [\"Congratulations, you won a free ticket!\"]\n",
        "new_pred = pipe.predict(new_text)\n",
        "print(\"\\nNew text:\", new_text[0])\n",
        "print(\"Predicted label:\", new_pred[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c9C2YFDPoZu",
        "outputId": "3bfb7d5b-efa1-4e32-8827-c4ce9efb8885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions: ['ham' 'ham']\n",
            "Actual labels:    ['spam', 'ham']\n",
            "Accuracy: 0.5\n",
            "\n",
            "New text: Congratulations, you won a free ticket!\n",
            "Predicted label: spam\n"
          ]
        }
      ]
    }
  ]
}